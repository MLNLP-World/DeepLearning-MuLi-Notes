## 58 深层循环神经网络

### 目录

  * [1.深层循环神经网络](#1深层循环神经网络)
  * [2.公式](#2公式)
  * [3.总结](#3总结)
  * [4.QA](#4qa)

### 1.深层循环神经网络

之前讲的RNN都只有一个隐藏层（序列变长不算是深度），而一个隐藏层的RNN一旦做的很宽就容易出现过拟合。因此我们考虑将网络做的更深而非更宽，每层都只做一点非线性，靠层数叠加得到更加非线性的模型。

浅RNN：输入-隐层-输出

深RNN：输入-隐层-隐层-...-输出

<img src="..\imgs\58\58-01.png" alt="58-01" style="zoom:25%;" />

（课程视频中的图片有错误，最后输出层后一时间步是不受前一步影响的，即没有箭头）

### 2.公式

<div align="center">
 
![](http://latex.codecogs.com/svg.latex?\mathbf{H}_t^1=f_1(\mathbf{H_{t-1}^1},\mathbf{X_t}))
 
</div>

*第一层的第t步状态是关于第一层第t-1步状态和第t步输入的函数*
<div align="center">
 
![](http://latex.codecogs.com/svg.latex?\mathbf{H}_t^j=f_j(\mathbf{H_{t-1}^j},\mathbf{H_{t}^{j-1})})
 
</div>
*第j层的第t步状态是关于当前层上一步步状态和上一层当前步的函数*

<div align="center">
 
![](http://latex.codecogs.com/svg.latex?\mathbf{O}_t=g(\mathbf{H}_t^L))
 
</div>
*由最后一个隐藏层得到输出*

### 3.总结

- 深度循环神经网络使用多个隐藏层来获得更多的非线性性

将RNN/GRU/LSTM做深都是一个道理，三者只是使用的函数f不同。

### 4.QA

Q1: NLP那个方向好找工作？文本翻译是不是现在只在学术研究中才需要自己实现？（2021-7-27）

> 文本翻译已经是一个很成熟的领域，NLP挺好找工作，人产生的文本远多于图片。

Q2: 关于BPTT

> 课上不讲，书上有讲原理

Q3: 深层RNN是不是每层都需要一个初始hidenstate?

> 是的

Q4: 可不可以手动实现hidden_size不一样的多层RNN？

> 应该没问题，但通常大家不会去调hidden_size，因为网络不会做的很深，最后还有全连接层。

Q5: 关于课上提到的classifier

> 分类的任务在最后的全连接层完成
